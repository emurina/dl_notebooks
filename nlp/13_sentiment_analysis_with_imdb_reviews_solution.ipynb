{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_sentiment_analysis_with_imdb_reviews_solution.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj0kayjfOfdv",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis with imdb reviews\n",
        "\n",
        "In this notebook we work with the IMDb dataset, it is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive (1) or negative (0). The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. We will apply a very simple preprocessing to the textreviews and then train a baseline randomforest on bag of words features. We will compare the results of the bag of word randomforest with a neural network performace where we learn a  dense word-embedding for each word and then classify it to either positive (1) or negative (0). Finally we will use an inception-like architecture with 1D convolutions and globalpooling and see if we can improve the performace. You can test the trained network on new reviews from the internet or by writting your own review for a movie you like or don't like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T30znDyfOfdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwiaITdXOfd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading the review and labels, if it does not exist\n",
        "import urllib\n",
        "import os\n",
        "if not os.path.isfile('movie_data.csv'):\n",
        "    urllib.request.urlretrieve(\n",
        "    \"https://www.dropbox.com/s/kvwi2nlrtk7axn9/movie_data.csv?dl=1\",\n",
        "    \"movie_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTuYfofUOfeB",
        "colab_type": "code",
        "outputId": "e31e974c-895d-41b5-955a-694f010d04a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  I went and saw this movie last night after bei...          1\n",
              "1  Actor turned director Bill Paxton follows up h...          1\n",
              "2  As a recreational golfer with some knowledge o...          1\n",
              "3  I saw this film in a sneak preview, and it is ...          1\n",
              "4  Bill Paxton has taken the true story of the 19...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhE_jotWOfeL",
        "colab_type": "text"
      },
      "source": [
        "shuffle the data and extract the first 15000 reviews with the sentiments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKY_bQmFOfeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.sample(frac=1,random_state=22).reset_index(drop=True)\n",
        "X = df.loc[0:24999, 'review'].values\n",
        "y = df.loc[0:24999, 'sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKm_KAAzOfeU",
        "colab_type": "text"
      },
      "source": [
        "look at one review before we apply some text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2VJcDPOfeV",
        "colab_type": "code",
        "outputId": "991de680-0519-4084-aead-a6f90dff4bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "i=0\n",
        "print(X[i])\n",
        "print(y[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not a new film. It is a re-cut of 1994's \"Emmanuelle, Queen of the Galaxy\", and it has been significantly truncated. Warning: Many characters appear in the credits that have been cut from the movie!<br /><br />If you want to see this one in its original form, pick up \"Queen\" - avoid this one at all costs, as the cuts make it even choppier than it was originally.\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnwg7vp6Ofec",
        "colab_type": "text"
      },
      "source": [
        "Here we do a very simple preprocessing, no stemming no lemmatization no stopwords removed  \n",
        "codecredit: https://stackabuse.com/text-classification-with-python-and-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0zbtIlOfed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = []\n",
        "\n",
        "for i in range(0, len(X)):  \n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[i]))\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    # Removing html stuff\n",
        "    document = re.sub(\"br\", '', document)\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    documents.append(document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8UKcGTOfeh",
        "colab_type": "text"
      },
      "source": [
        "look at the review from before after the text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gQyS5HROfej",
        "colab_type": "code",
        "outputId": "09536c05-eeed-45a6-a19a-7d19ab99c199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "documents[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is not new film it is re cut of 1994 emmanuelle queen of the galaxy and it has been significantly truncated warning many characters appear in the credits that have been cut from the movie if you want to see this one in its original form pick up queen avoid this one at all costs as the cuts make it even choppier than it was originally '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFpB4WTOfeu",
        "colab_type": "text"
      },
      "source": [
        "## RF Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEFN9cp2Ofet",
        "colab_type": "text"
      },
      "source": [
        "Here we \"vectorize\" the bag of words. We choose to consider only the 6000 most frequent words and take only words that appear in at least 5 diffrent reviews, furthermore we ignore all words that appear in more than 70% of all reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keboNZRTOfev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "vectorizer = CountVectorizer(max_features=6000, min_df=5, max_df=0.7)\n",
        " \n",
        "X = vectorizer.fit_transform(documents).toarray()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KVtAwNMOfez",
        "colab_type": "code",
        "outputId": "32a5dac2-e497-40e1-b931-f0f87930b998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "print(len(vectorizer.get_feature_names()))#length of all tokens\n",
        "np.array(vectorizer.get_feature_names()[0:200])#the first 200 tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '10', '100', '11', '12', '13', '13th', '14', '15',\n",
              "       '16', '17', '18', '1930', '1930s', '1933', '1939', '1940', '1940s',\n",
              "       '1945', '1950', '1950s', '1960', '1960s', '1968', '1969', '1970',\n",
              "       '1970s', '1971', '1972', '1973', '1976', '1977', '1978', '1979',\n",
              "       '1980', '1980s', '1981', '1983', '1984', '1986', '1987', '1988',\n",
              "       '1989', '1990', '1993', '1994', '1995', '1996', '1997', '1999',\n",
              "       '19th', '1st', '20', '2000', '2001', '2002', '2003', '2004',\n",
              "       '2005', '2006', '2007', '2008', '20th', '21st', '22', '23', '24',\n",
              "       '25', '2nd', '30', '3000', '30s', '35', '3d', '3rd', '40', '40s',\n",
              "       '45', '50', '50s', '60', '60s', '70', '70s', '75', '80', '80s',\n",
              "       '90', '90s', '95', '99', 'aaron', 'abandoned', 'abc', 'abilities',\n",
              "       'ability', 'able', 'aboard', 'about', 'above', 'absence', 'absent',\n",
              "       'absolute', 'absolutely', 'absurd', 'abuse', 'abused', 'abusive',\n",
              "       'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable',\n",
              "       'acceptance', 'accepted', 'access', 'accident', 'accidentally',\n",
              "       'acclaimed', 'accompanied', 'accomplish', 'accomplished',\n",
              "       'according', 'account', 'accuracy', 'accurate', 'accurately',\n",
              "       'accused', 'ace', 'achieve', 'achieved', 'achievement', 'acid',\n",
              "       'across', 'act', 'acted', 'acting', 'action', 'actions', 'active',\n",
              "       'activities', 'activity', 'actor', 'actors', 'actress',\n",
              "       'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams',\n",
              "       'adaptation', 'adaptations', 'adapted', 'add', 'added', 'addict',\n",
              "       'addicted', 'addiction', 'adding', 'addition', 'additional',\n",
              "       'address', 'adds', 'adequate', 'admirable', 'admire', 'admission',\n",
              "       'admit', 'admittedly', 'adolescent', 'adopted', 'adorable',\n",
              "       'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure',\n",
              "       'adventures', 'advertising', 'advice', 'advise', 'aesthetic',\n",
              "       'affair', 'affairs', 'affect', 'affected', 'affection', 'afford',\n",
              "       'aforementioned', 'afraid', 'africa', 'african', 'after',\n",
              "       'afternoon', 'afterwards'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv6UVT8FOfe4",
        "colab_type": "text"
      },
      "source": [
        "spliting the bag of words into a train valid and testset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VW4EENZOfe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X[0:10000]\n",
        "y_train = y[0:10000]\n",
        "X_val = X[10000:20000]\n",
        "y_val = y[10000:20000]\n",
        "X_test = X[20000:25000]\n",
        "y_test = y[20000:25000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8egWs4JOfe-",
        "colab_type": "text"
      },
      "source": [
        "the data more or less balanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkjuPtSGOfe_",
        "colab_type": "code",
        "outputId": "46ea640d-3c2a-4ef7-e17c-04dfe35d2e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_val,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0, 1]), array([4994, 5006]))\n",
            "(array([0, 1]), array([4965, 5035]))\n",
            "(array([0, 1]), array([2507, 2493]))\n",
            "(10000,)\n",
            "(10000,)\n",
            "(5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLT32ZjeOffF",
        "colab_type": "text"
      },
      "source": [
        "bag of words for the first five reviews (very sparse representation), 5000 observations, with 6000 features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nrYqisOffH",
        "colab_type": "code",
        "outputId": "c991f1af-4f86-41d8-ce4e-5a7ed6e3af29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(X_train[0:5])\n",
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This is not a new film. It is a re-cut of 1994\\'s \"Emmanuelle, Queen of the Galaxy\", and it has been significantly truncated. Warning: Many characters appear in the credits that have been cut from the movie!<br /><br />If you want to see this one in its original form, pick up \"Queen\" - avoid this one at all costs, as the cuts make it even choppier than it was originally.'\n",
            " 'I can only guess that this movie was an experiment that misfired. Years earlier, it would have been moving images accompanied by music. Later, it would have been sound added to silents. Eventually it would have been Technicolor, Cinemascope or Imax. This movie must have been a misguided attempt to introduce a new element to the talking picture. During all the emotional scenes, the character stops in mid dialogue and their inner thoughts are narrated while they gaze off into the distance or appeal to the camera. This interruption is painful at it\\'s very least. Imagine these top tier actors trying to look busy while the narration drones on. Painful. I have no idea who came up with this gimmick, but it was the only time I ever saw it used - and for good reason. In every scene the actors were forced to roll their eyes, wring their hands, or overact to such a degree, I actually wondered if this was really a comedy. <br /><br />The story is a hopeless soap opera that takes place over a couple of generations. Norma Shearer, disappointed in love, searches for a reason to live. She has a friend, played by Ralph Morgan, who worships her - but she takes him for granted. She is attracted to a doctor, played by Clark Gable, but he is self absorbed and isn\\'t interested in her. She settles for a weakling that needs her desperately. She marries him only to find that there is insanity in his family and she can never have a child with him. Along comes the doctor who selfishly pops a bun in her oven, only to find out later that he loves her after all. The child builds confidence in her husband who becomes a success, but she realizes that it\\'s really Clark she loves after all. Confused yet? Forget the rest, just watch a couple of episodes of \"As the World Turns\" and it\\'ll all become clear.<br /><br />If your are ever forced to watch this movie, hold out for the final scene. The gyrations of the actors put Harold LLoyd to shame. It is not to be missed.'\n",
            " 'I have loved the book \"A Little Princess\" for most of my life, and was very excited that there was a movie. But I was appalled at this adaptation. Not only is the acting wooden, and the plot a convoluted mish mash of various incidents in the book, but the theme is all wrong. The real theme of the story should be that a girl can be a princess only when she behaves like one, as Sara does when she gives 5 of her 6 buns to a beggar child, even when she herself is very hungry. The theme of the movie seems to be that all girls are princesses, which cheapens Sara\\'s actions considerably, and seems more like it should be written on a Hallmark card than applied to this story.<br /><br />There are many other things wrong with this movie- too many to list, but here are just a few of the larger ones: This story should be set in Britian in the mid 1800s, not America during the first world war. Miss Minchen is harsh to Sara from the start, making her actions when Sara is left penniless much less startling than they would be if she was syrupy sweet at the beginning, as she is supposed to be. Nowhere is it mentioned that Becky is black. Sara\\'s father does *not* come back, he is dead. It is his closest friend, and collaborator in the diamond mines who finds Sara, and restores her to her proper place. In fact, the diamond mines are not even mentioned at all, though they are the source of Sara\\'s wealth.<br /><br />All through everything that Sara has faced, she always acts like a Princess, giving what she can, and forgiving those who hurt her. She would never have called Lavinia a \"snotty two faced bully\". Such a thing is completely out of character for her, and undermines the entire philosophy that she is to be well behaved no matter what.<br /><br />This is by far the worst adaptation of a book to the screen that I have ever seen (with the notable exceptions of \"Ella Enchanted\", and \"Anne of Green Gables the Continuing Story\")The plot of the book is wonderful, and skillfully written, so I do not understand why the director felt that it needed to be changed to make it interesting. I would suggest that anyone wishing to know this story should watch the 1987 version, which is far superior. Or better still, read the book. It will be more worth your time than the hour and a half wasted on this version on the movie.'\n",
            " 'I was dreading taking my nephews to this movie, as I didn\\'t think it was going to be well done. The kids, ages 6 and 10 were set on seeing it, so I caved. I must admit that it was not nearly as bad as I had thought, but was still a far cry from the book. The movie seemed right on with the 10 year old\\'s understanding and sense of humor. I found that the 6 year old understood what was going on and he was presenting solutions to the issues that were taking place. I eventually had to explain that sometimes the movies don\\'t show the best solutions to the problems because it is more fun to watch what happens if they make the \"silly\" or \"stupid\" choices.'\n",
            " '<br /><br />\"Burning Paradise\" is a combination of neo-Shaw Brothers action and Ringo Lam\\'s urban cynicism. When one watches the film, they might feel the fight scenes are only mediocre in nature but that doesn\\'t matter, it\\'s attitude and atmosphere that counts. This great film has both!! Always trying to be different than his contemporaries, Lam gives us to traditional heroes(Fong Sai-Yuk and Hung Shi-Kwan)and puts them in a \"Raiders of the Lost Ark\" setting. However, these are not the light-hearted comedic incarnations that you might see in a Jet Li movie. Instead these guys fight to the death with brutal results. What makes the film even better is that anyone could die at anytime, there is no holding back. Too bad, they don\\'t make films like this more often.']\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OySVMPm_OffM",
        "colab_type": "text"
      },
      "source": [
        "let's train a radomforest on the bag of words features of the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqBynwKWOffQ",
        "colab_type": "code",
        "outputId": "e1181867-a20b-40b3-b97f-7a55a579ffee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=300, random_state=36)  \n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
              "                       n_jobs=None, oob_score=False, random_state=36, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odquYZ9rOffY",
        "colab_type": "text"
      },
      "source": [
        "prediction on the test set with the accuracy and the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp5ZBTfpOffZ",
        "colab_type": "code",
        "outputId": "98d88d31-59f8-480c-c054-719fd26d4ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "y_pred = classifier.predict(X_test)  \n",
        "print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n",
        "print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred     0     1\n",
            "true            \n",
            "0     2084   423\n",
            "1      389  2104\n",
            "Acc =  0.8376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSu20wPwOffe",
        "colab_type": "text"
      },
      "source": [
        "## Neural network with wordembedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMe0a0GOffg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = documents[0:10000]\n",
        "y_train = y[0:10000]\n",
        "X_val = documents[10000:20000]\n",
        "y_val = y[10000:20000]\n",
        "X_test = documents[20000:25000]\n",
        "y_test = y[20000:25000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sEMGLVWOffj",
        "colab_type": "code",
        "outputId": "a78de99c-553f-4eaa-a18f-2e196aa7b041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is not new film it is re cut of 1994 emmanuelle queen of the galaxy and it has been significantly truncated warning many characters appear in the credits that have been cut from the movie if you want to see this one in its original form pick up queen avoid this one at all costs as the cuts make it even choppier than it was originally '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXv7H2cgOffm",
        "colab_type": "text"
      },
      "source": [
        "In the next cell we tokenize all unique words in the reviews and transform them into a sequence of the corresponding integer number of the token that belongs to the word. For example we cound transform \"the\" into the number 7. Then we take the length of the longest review and we zeropad all other reviews to that length of the longest review, so all reviews have the same length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G35l_6qjOffo",
        "colab_type": "code",
        "outputId": "77dc0d36-e088-4a51-9369-86db8566c28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# fit tokenizer on all reviews\n",
        "total_reviews = documents\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(total_reviews) \n",
        "\n",
        "\n",
        "# transform tokens to a sequence of integers\n",
        "X_train_tokens =  tokenizer.texts_to_sequences(X_train)\n",
        "X_val_tokens = tokenizer.texts_to_sequences(X_val)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# max length of all reviews\n",
        "max_length = max([len(s.split()) for s in total_reviews])\n",
        "print(\"longest review:\",max_length,\"words\")\n",
        "\n",
        "# define vocabulary size (unique words in all reviews)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"number of unique words/tokens:\",vocab_size)\n",
        "\n",
        "\n",
        "# zeropad the sequences to have the \"same\" length \n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "longest review: 2381 words\n",
            "number of unique words/tokens: 76912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib7farhIOffv",
        "colab_type": "text"
      },
      "source": [
        "this is how our input for the neural network will look like, just a sequence of integer numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9CRiTztOffw",
        "colab_type": "code",
        "outputId": "a5c46294-ab30-4444-93f1-433a3a84c690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_pad[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8,  5, 19, ...,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPiIV1SMOff2",
        "colab_type": "text"
      },
      "source": [
        "all unique tokens with the corresponding number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fskOK_MOff3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx-QPpLjOff7",
        "colab_type": "text"
      },
      "source": [
        "definition of the network with an embedding layer in the input that maps the numbers (words) into vectors of the same size.\n",
        "architecture inspired by: https://www.tensorflow.org/tutorials/keras/basic_text_classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCNxC8sZOff8",
        "colab_type": "code",
        "outputId": "c35c90ae-1613-41c2-fe26-04b49759d5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding,GlobalAveragePooling1D,Dropout\n",
        "\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=(None)))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 30)          2307360   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 2,308,001\n",
            "Trainable params: 2,308,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1WITi5IOfgA",
        "colab_type": "code",
        "outputId": "f255fc14-6d8b-4d0c-cc67-5586f297b33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2754
        }
      },
      "source": [
        "history_no_dropout=model.fit(X_train_pad, y_train, batch_size=64, epochs=80, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/80\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.6929 - acc: 0.5013 - val_loss: 0.6920 - val_acc: 0.4984\n",
            "Epoch 2/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.6897 - acc: 0.5467 - val_loss: 0.6874 - val_acc: 0.5095\n",
            "Epoch 3/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.6777 - acc: 0.6062 - val_loss: 0.6654 - val_acc: 0.6290\n",
            "Epoch 4/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.6255 - acc: 0.7253 - val_loss: 0.5934 - val_acc: 0.6984\n",
            "Epoch 5/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.5236 - acc: 0.8018 - val_loss: 0.4987 - val_acc: 0.8082\n",
            "Epoch 6/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.4299 - acc: 0.8522 - val_loss: 0.4330 - val_acc: 0.8403\n",
            "Epoch 7/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.3627 - acc: 0.8722 - val_loss: 0.3862 - val_acc: 0.8544\n",
            "Epoch 8/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.3121 - acc: 0.8929 - val_loss: 0.3544 - val_acc: 0.8652\n",
            "Epoch 9/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.2682 - acc: 0.9115 - val_loss: 0.3350 - val_acc: 0.8686\n",
            "Epoch 10/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.2369 - acc: 0.9220 - val_loss: 0.3157 - val_acc: 0.8785\n",
            "Epoch 11/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.2112 - acc: 0.9317 - val_loss: 0.3161 - val_acc: 0.8722\n",
            "Epoch 12/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.1922 - acc: 0.9382 - val_loss: 0.3032 - val_acc: 0.8794\n",
            "Epoch 13/80\n",
            "10000/10000 [==============================] - 1s 137us/step - loss: 0.1713 - acc: 0.9459 - val_loss: 0.2916 - val_acc: 0.8850\n",
            "Epoch 14/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.1551 - acc: 0.9496 - val_loss: 0.2930 - val_acc: 0.8838\n",
            "Epoch 15/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.1404 - acc: 0.9558 - val_loss: 0.2886 - val_acc: 0.8860\n",
            "Epoch 16/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.1271 - acc: 0.9615 - val_loss: 0.2904 - val_acc: 0.8849\n",
            "Epoch 17/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.1168 - acc: 0.9631 - val_loss: 0.2863 - val_acc: 0.8873\n",
            "Epoch 18/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.1071 - acc: 0.9693 - val_loss: 0.2968 - val_acc: 0.8846\n",
            "Epoch 19/80\n",
            "10000/10000 [==============================] - 1s 137us/step - loss: 0.0950 - acc: 0.9723 - val_loss: 0.2865 - val_acc: 0.8885\n",
            "Epoch 20/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0862 - acc: 0.9756 - val_loss: 0.2890 - val_acc: 0.8882\n",
            "Epoch 21/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0792 - acc: 0.9787 - val_loss: 0.2904 - val_acc: 0.8877\n",
            "Epoch 22/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0722 - acc: 0.9822 - val_loss: 0.2932 - val_acc: 0.8897\n",
            "Epoch 23/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0665 - acc: 0.9829 - val_loss: 0.3168 - val_acc: 0.8827\n",
            "Epoch 24/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.0637 - acc: 0.9827 - val_loss: 0.3023 - val_acc: 0.8881\n",
            "Epoch 25/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0524 - acc: 0.9891 - val_loss: 0.3018 - val_acc: 0.8885\n",
            "Epoch 26/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0472 - acc: 0.9891 - val_loss: 0.3203 - val_acc: 0.8858\n",
            "Epoch 27/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.0429 - acc: 0.9915 - val_loss: 0.3103 - val_acc: 0.8894\n",
            "Epoch 28/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.3222 - val_acc: 0.8869\n",
            "Epoch 29/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0362 - acc: 0.9930 - val_loss: 0.3302 - val_acc: 0.8874\n",
            "Epoch 30/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0337 - acc: 0.9933 - val_loss: 0.3298 - val_acc: 0.8884\n",
            "Epoch 31/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.0286 - acc: 0.9953 - val_loss: 0.3330 - val_acc: 0.8880\n",
            "Epoch 32/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0280 - acc: 0.9953 - val_loss: 0.3391 - val_acc: 0.8878\n",
            "Epoch 33/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0235 - acc: 0.9965 - val_loss: 0.3411 - val_acc: 0.8880\n",
            "Epoch 34/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0217 - acc: 0.9970 - val_loss: 0.3484 - val_acc: 0.8880\n",
            "Epoch 35/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0189 - acc: 0.9981 - val_loss: 0.3540 - val_acc: 0.8886\n",
            "Epoch 36/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0180 - acc: 0.9981 - val_loss: 0.3594 - val_acc: 0.8892\n",
            "Epoch 37/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0165 - acc: 0.9981 - val_loss: 0.3682 - val_acc: 0.8869\n",
            "Epoch 38/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0158 - acc: 0.9975 - val_loss: 0.3710 - val_acc: 0.8884\n",
            "Epoch 39/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0143 - acc: 0.9979 - val_loss: 0.3938 - val_acc: 0.8820\n",
            "Epoch 40/80\n",
            "10000/10000 [==============================] - 1s 137us/step - loss: 0.0117 - acc: 0.9988 - val_loss: 0.4031 - val_acc: 0.8819\n",
            "Epoch 41/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0115 - acc: 0.9988 - val_loss: 0.4459 - val_acc: 0.8722\n",
            "Epoch 42/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0102 - acc: 0.9989 - val_loss: 0.3958 - val_acc: 0.8882\n",
            "Epoch 43/80\n",
            "10000/10000 [==============================] - 1s 136us/step - loss: 0.0096 - acc: 0.9991 - val_loss: 0.4035 - val_acc: 0.8859\n",
            "Epoch 44/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0077 - acc: 0.9994 - val_loss: 0.4105 - val_acc: 0.8858\n",
            "Epoch 45/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.0070 - acc: 0.9997 - val_loss: 0.4200 - val_acc: 0.8853\n",
            "Epoch 46/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0064 - acc: 0.9995 - val_loss: 0.4262 - val_acc: 0.8855\n",
            "Epoch 47/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0063 - acc: 0.9997 - val_loss: 0.4287 - val_acc: 0.8864\n",
            "Epoch 48/80\n",
            "10000/10000 [==============================] - 1s 135us/step - loss: 0.0055 - acc: 0.9999 - val_loss: 0.4493 - val_acc: 0.8810\n",
            "Epoch 49/80\n",
            "10000/10000 [==============================] - 1s 129us/step - loss: 0.0045 - acc: 0.9999 - val_loss: 0.4441 - val_acc: 0.8851\n",
            "Epoch 50/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 0.0044 - acc: 0.9998 - val_loss: 0.4472 - val_acc: 0.8858\n",
            "Epoch 51/80\n",
            "10000/10000 [==============================] - 1s 127us/step - loss: 0.0043 - acc: 0.9999 - val_loss: 0.4559 - val_acc: 0.8854\n",
            "Epoch 52/80\n",
            "10000/10000 [==============================] - 1s 137us/step - loss: 0.0039 - acc: 0.9999 - val_loss: 0.4613 - val_acc: 0.8849\n",
            "Epoch 53/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4683 - val_acc: 0.8847\n",
            "Epoch 54/80\n",
            "10000/10000 [==============================] - 1s 134us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.4926 - val_acc: 0.8793\n",
            "Epoch 55/80\n",
            "10000/10000 [==============================] - 1s 133us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.4848 - val_acc: 0.8841\n",
            "Epoch 56/80\n",
            "10000/10000 [==============================] - 1s 129us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4870 - val_acc: 0.8845\n",
            "Epoch 57/80\n",
            "10000/10000 [==============================] - 1s 124us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8832\n",
            "Epoch 58/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4993 - val_acc: 0.8849\n",
            "Epoch 59/80\n",
            "10000/10000 [==============================] - 1s 124us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5081 - val_acc: 0.8838\n",
            "Epoch 60/80\n",
            "10000/10000 [==============================] - 1s 125us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.5129 - val_acc: 0.8846\n",
            "Epoch 61/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5203 - val_acc: 0.8836\n",
            "Epoch 62/80\n",
            "10000/10000 [==============================] - 1s 128us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.5263 - val_acc: 0.8826\n",
            "Epoch 63/80\n",
            "10000/10000 [==============================] - 1s 128us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5314 - val_acc: 0.8839\n",
            "Epoch 64/80\n",
            "10000/10000 [==============================] - 1s 129us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5374 - val_acc: 0.8831\n",
            "Epoch 65/80\n",
            "10000/10000 [==============================] - 1s 127us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.5781 - val_acc: 0.8771\n",
            "Epoch 66/80\n",
            "10000/10000 [==============================] - 1s 127us/step - loss: 9.6656e-04 - acc: 1.0000 - val_loss: 0.5516 - val_acc: 0.8821\n",
            "Epoch 67/80\n",
            "10000/10000 [==============================] - 1s 127us/step - loss: 8.9379e-04 - acc: 1.0000 - val_loss: 0.6097 - val_acc: 0.8736\n",
            "Epoch 68/80\n",
            "10000/10000 [==============================] - 1s 128us/step - loss: 7.8550e-04 - acc: 1.0000 - val_loss: 0.5897 - val_acc: 0.8762\n",
            "Epoch 69/80\n",
            "10000/10000 [==============================] - 1s 128us/step - loss: 7.6550e-04 - acc: 1.0000 - val_loss: 0.5709 - val_acc: 0.8824\n",
            "Epoch 70/80\n",
            "10000/10000 [==============================] - 1s 125us/step - loss: 6.1345e-04 - acc: 1.0000 - val_loss: 0.5819 - val_acc: 0.8815\n",
            "Epoch 71/80\n",
            "10000/10000 [==============================] - 1s 124us/step - loss: 5.6811e-04 - acc: 1.0000 - val_loss: 0.5803 - val_acc: 0.8821\n",
            "Epoch 72/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 5.8162e-04 - acc: 1.0000 - val_loss: 0.5866 - val_acc: 0.8812\n",
            "Epoch 73/80\n",
            "10000/10000 [==============================] - 1s 124us/step - loss: 4.4241e-04 - acc: 1.0000 - val_loss: 0.5959 - val_acc: 0.8814\n",
            "Epoch 74/80\n",
            "10000/10000 [==============================] - 1s 122us/step - loss: 4.6934e-04 - acc: 1.0000 - val_loss: 0.6154 - val_acc: 0.8772\n",
            "Epoch 75/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 4.4939e-04 - acc: 1.0000 - val_loss: 0.6048 - val_acc: 0.8812\n",
            "Epoch 76/80\n",
            "10000/10000 [==============================] - 1s 124us/step - loss: 3.5033e-04 - acc: 1.0000 - val_loss: 0.6104 - val_acc: 0.8808\n",
            "Epoch 77/80\n",
            "10000/10000 [==============================] - 1s 122us/step - loss: 3.3533e-04 - acc: 1.0000 - val_loss: 0.6151 - val_acc: 0.8813\n",
            "Epoch 78/80\n",
            "10000/10000 [==============================] - 1s 125us/step - loss: 3.1884e-04 - acc: 1.0000 - val_loss: 0.6293 - val_acc: 0.8798\n",
            "Epoch 79/80\n",
            "10000/10000 [==============================] - 1s 123us/step - loss: 3.1350e-04 - acc: 1.0000 - val_loss: 0.6269 - val_acc: 0.8813\n",
            "Epoch 80/80\n",
            "10000/10000 [==============================] - 1s 122us/step - loss: 2.8047e-04 - acc: 1.0000 - val_loss: 0.6442 - val_acc: 0.8795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj50I40pr8yK",
        "colab_type": "code",
        "outputId": "3cec6acd-e9fb-4964-c8cd-401d13ea2220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding,GlobalAveragePooling1D,Dropout\n",
        "\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=(None)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(GlobalAv\n",
        "())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, None, 30)          2307360   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_26 (Glo (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 2,308,001\n",
            "Trainable params: 2,308,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f446059d-756a-4e82-c7d4-7e0af7586be8",
        "id": "7eg2EjEqr_cY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2754
        }
      },
      "source": [
        "history_with_dropout=model.fit(X_train_pad, y_train, batch_size=64, epochs=80, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/80\n",
            "10000/10000 [==============================] - 3s 291us/step - loss: 0.6919 - acc: 0.5196 - val_loss: 0.6858 - val_acc: 0.7229\n",
            "Epoch 2/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.6764 - acc: 0.5771 - val_loss: 0.6391 - val_acc: 0.7824\n",
            "Epoch 3/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.6219 - acc: 0.6593 - val_loss: 0.5337 - val_acc: 0.8013\n",
            "Epoch 4/80\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.5543 - acc: 0.7265 - val_loss: 0.4627 - val_acc: 0.8139\n",
            "Epoch 5/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.5035 - acc: 0.7666 - val_loss: 0.4260 - val_acc: 0.8278\n",
            "Epoch 6/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.4703 - acc: 0.7956 - val_loss: 0.4034 - val_acc: 0.8373\n",
            "Epoch 7/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.4286 - acc: 0.8131 - val_loss: 0.3778 - val_acc: 0.8433\n",
            "Epoch 8/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.4110 - acc: 0.8313 - val_loss: 0.3709 - val_acc: 0.8477\n",
            "Epoch 9/80\n",
            "10000/10000 [==============================] - 1s 149us/step - loss: 0.3873 - acc: 0.8441 - val_loss: 0.3608 - val_acc: 0.8503\n",
            "Epoch 10/80\n",
            "10000/10000 [==============================] - 1s 146us/step - loss: 0.3640 - acc: 0.8518 - val_loss: 0.3552 - val_acc: 0.8505\n",
            "Epoch 11/80\n",
            "10000/10000 [==============================] - 1s 147us/step - loss: 0.3364 - acc: 0.8705 - val_loss: 0.3500 - val_acc: 0.8527\n",
            "Epoch 12/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.3074 - acc: 0.8812 - val_loss: 0.3498 - val_acc: 0.8485\n",
            "Epoch 13/80\n",
            "10000/10000 [==============================] - 1s 145us/step - loss: 0.2961 - acc: 0.8870 - val_loss: 0.3461 - val_acc: 0.8489\n",
            "Epoch 14/80\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.2751 - acc: 0.8978 - val_loss: 0.3441 - val_acc: 0.8505\n",
            "Epoch 15/80\n",
            "10000/10000 [==============================] - 1s 147us/step - loss: 0.2716 - acc: 0.8981 - val_loss: 0.3445 - val_acc: 0.8489\n",
            "Epoch 16/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.2447 - acc: 0.9113 - val_loss: 0.3443 - val_acc: 0.8509\n",
            "Epoch 17/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.2359 - acc: 0.9125 - val_loss: 0.3466 - val_acc: 0.8483\n",
            "Epoch 18/80\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.2209 - acc: 0.9175 - val_loss: 0.3483 - val_acc: 0.8469\n",
            "Epoch 19/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.2087 - acc: 0.9265 - val_loss: 0.3524 - val_acc: 0.8482\n",
            "Epoch 20/80\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.1973 - acc: 0.9291 - val_loss: 0.3539 - val_acc: 0.8457\n",
            "Epoch 21/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.1902 - acc: 0.9312 - val_loss: 0.3556 - val_acc: 0.8455\n",
            "Epoch 22/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.1855 - acc: 0.9322 - val_loss: 0.3584 - val_acc: 0.8442\n",
            "Epoch 23/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.1650 - acc: 0.9432 - val_loss: 0.3610 - val_acc: 0.8444\n",
            "Epoch 24/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1683 - acc: 0.9418 - val_loss: 0.3691 - val_acc: 0.8403\n",
            "Epoch 25/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.1550 - acc: 0.9420 - val_loss: 0.3714 - val_acc: 0.8420\n",
            "Epoch 26/80\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.1477 - acc: 0.9485 - val_loss: 0.3772 - val_acc: 0.8401\n",
            "Epoch 27/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1311 - acc: 0.9551 - val_loss: 0.3861 - val_acc: 0.8388\n",
            "Epoch 28/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.1314 - acc: 0.9541 - val_loss: 0.3904 - val_acc: 0.8369\n",
            "Epoch 29/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1199 - acc: 0.9596 - val_loss: 0.4001 - val_acc: 0.8354\n",
            "Epoch 30/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1169 - acc: 0.9589 - val_loss: 0.4031 - val_acc: 0.8342\n",
            "Epoch 31/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.1205 - acc: 0.9587 - val_loss: 0.4081 - val_acc: 0.8320\n",
            "Epoch 32/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1120 - acc: 0.9617 - val_loss: 0.4079 - val_acc: 0.8323\n",
            "Epoch 33/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1033 - acc: 0.9647 - val_loss: 0.4182 - val_acc: 0.8295\n",
            "Epoch 34/80\n",
            "10000/10000 [==============================] - 1s 142us/step - loss: 0.1067 - acc: 0.9649 - val_loss: 0.4241 - val_acc: 0.8276\n",
            "Epoch 35/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.1025 - acc: 0.9682 - val_loss: 0.4311 - val_acc: 0.8287\n",
            "Epoch 36/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0954 - acc: 0.9688 - val_loss: 0.4367 - val_acc: 0.8260\n",
            "Epoch 37/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0803 - acc: 0.9736 - val_loss: 0.4542 - val_acc: 0.8235\n",
            "Epoch 38/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0793 - acc: 0.9739 - val_loss: 0.4605 - val_acc: 0.8220\n",
            "Epoch 39/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0817 - acc: 0.9721 - val_loss: 0.4598 - val_acc: 0.8206\n",
            "Epoch 40/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0840 - acc: 0.9725 - val_loss: 0.4675 - val_acc: 0.8190\n",
            "Epoch 41/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0739 - acc: 0.9762 - val_loss: 0.4752 - val_acc: 0.8176\n",
            "Epoch 42/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0708 - acc: 0.9764 - val_loss: 0.4876 - val_acc: 0.8160\n",
            "Epoch 43/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0680 - acc: 0.9778 - val_loss: 0.5023 - val_acc: 0.8114\n",
            "Epoch 44/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0631 - acc: 0.9803 - val_loss: 0.5066 - val_acc: 0.8134\n",
            "Epoch 45/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0651 - acc: 0.9769 - val_loss: 0.5081 - val_acc: 0.8116\n",
            "Epoch 46/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0559 - acc: 0.9824 - val_loss: 0.5236 - val_acc: 0.8110\n",
            "Epoch 47/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0593 - acc: 0.9802 - val_loss: 0.5285 - val_acc: 0.8119\n",
            "Epoch 48/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0559 - acc: 0.9823 - val_loss: 0.5365 - val_acc: 0.8115\n",
            "Epoch 49/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0629 - acc: 0.9795 - val_loss: 0.5276 - val_acc: 0.8108\n",
            "Epoch 50/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0564 - acc: 0.9819 - val_loss: 0.5383 - val_acc: 0.8075\n",
            "Epoch 51/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0475 - acc: 0.9858 - val_loss: 0.5594 - val_acc: 0.8052\n",
            "Epoch 52/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0447 - acc: 0.9841 - val_loss: 0.5710 - val_acc: 0.8046\n",
            "Epoch 53/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0451 - acc: 0.9857 - val_loss: 0.5822 - val_acc: 0.8018\n",
            "Epoch 54/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0473 - acc: 0.9846 - val_loss: 0.5701 - val_acc: 0.8031\n",
            "Epoch 55/80\n",
            "10000/10000 [==============================] - 1s 137us/step - loss: 0.0425 - acc: 0.9877 - val_loss: 0.6010 - val_acc: 0.7986\n",
            "Epoch 56/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0392 - acc: 0.9874 - val_loss: 0.6144 - val_acc: 0.8021\n",
            "Epoch 57/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0393 - acc: 0.9875 - val_loss: 0.6218 - val_acc: 0.7998\n",
            "Epoch 58/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0406 - acc: 0.9849 - val_loss: 0.6283 - val_acc: 0.7953\n",
            "Epoch 59/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0386 - acc: 0.9880 - val_loss: 0.6271 - val_acc: 0.7968\n",
            "Epoch 60/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0333 - acc: 0.9893 - val_loss: 0.6505 - val_acc: 0.7932\n",
            "Epoch 61/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0359 - acc: 0.9896 - val_loss: 0.6532 - val_acc: 0.7924\n",
            "Epoch 62/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.6518 - val_acc: 0.7941\n",
            "Epoch 63/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0370 - acc: 0.9898 - val_loss: 0.6505 - val_acc: 0.7923\n",
            "Epoch 64/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0284 - acc: 0.9921 - val_loss: 0.6759 - val_acc: 0.7923\n",
            "Epoch 65/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0317 - acc: 0.9908 - val_loss: 0.6752 - val_acc: 0.7916\n",
            "Epoch 66/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.6977 - val_acc: 0.7898\n",
            "Epoch 67/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.6917 - val_acc: 0.7881\n",
            "Epoch 68/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0271 - acc: 0.9928 - val_loss: 0.7117 - val_acc: 0.7853\n",
            "Epoch 69/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0260 - acc: 0.9919 - val_loss: 0.7274 - val_acc: 0.7837\n",
            "Epoch 70/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0294 - acc: 0.9914 - val_loss: 0.7142 - val_acc: 0.7840\n",
            "Epoch 71/80\n",
            "10000/10000 [==============================] - 1s 138us/step - loss: 0.0259 - acc: 0.9919 - val_loss: 0.7349 - val_acc: 0.7835\n",
            "Epoch 72/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.7520 - val_acc: 0.7833\n",
            "Epoch 73/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.7775 - val_acc: 0.7820\n",
            "Epoch 74/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0270 - acc: 0.9922 - val_loss: 0.7598 - val_acc: 0.7814\n",
            "Epoch 75/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0227 - acc: 0.9927 - val_loss: 0.7681 - val_acc: 0.7799\n",
            "Epoch 76/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.7745 - val_acc: 0.7754\n",
            "Epoch 77/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0203 - acc: 0.9932 - val_loss: 0.7890 - val_acc: 0.7768\n",
            "Epoch 78/80\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0238 - acc: 0.9937 - val_loss: 0.7859 - val_acc: 0.7799\n",
            "Epoch 79/80\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.8127 - val_acc: 0.7798\n",
            "Epoch 80/80\n",
            "10000/10000 [==============================] - 1s 139us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.8092 - val_acc: 0.7762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmG-_oo3r8vT",
        "colab_type": "code",
        "outputId": "74cbf768-086e-42ae-97f4-31d164b5c724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# summarize history \n",
        "plt.plot(history_no_dropout.history['val_acc'],color=\"blue\")\n",
        "plt.plot(history_with_dropout.history['val_acc'],color=\"red\")\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['wo_dropo', 'w_dropo'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_no_dropout.history['val_loss'],color=\"blue\")\n",
        "plt.plot(history_with_dropout.history['val_loss'],color=\"red\")\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['wo_dropo', 'w_dropo'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raNGqCQJtwVR",
        "colab_type": "text"
      },
      "source": [
        "#### Performace is almost the same around 88% acc, the model with dropout seems to overfit a bit more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsMpPPwOfgJ",
        "colab_type": "code",
        "outputId": "90770ac8-ccab-480d-8859-b684b0d87026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "y_pred = model.predict_classes(X_test_pad)  \n",
        "y_pred=y_pred.reshape(len(y_pred))\n",
        "print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n",
        "print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred     0     1\n",
            "true            \n",
            "0     2218   289\n",
            "1      274  2219\n",
            "Acc =  0.8874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vqy51AWuDCI",
        "colab_type": "text"
      },
      "source": [
        "## model with 1d conv after emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnak5Px9r8r7",
        "colab_type": "code",
        "outputId": "bd608bd1-638c-4cc4-f0d8-6b3d86acf454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation,Dense, Concatenate, Dropout, Embedding, Conv1D,GlobalMaxPooling1D, Flatten ,MaxPooling1D, GlobalAvgPool1D,BatchNormalization\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "\n",
        "a = Input(shape=(max_length,))\n",
        "\n",
        "x = Embedding(vocab_size+ 1, EMBEDDING_DIM)(a)\n",
        "x1 = Conv1D(filters=20,kernel_size=(3),activation=\"relu\",padding=\"same\")(x)\n",
        "#x1= Dropout(0.2)(x1)\n",
        "x2 = Conv1D(filters=20,kernel_size=(5),activation=\"relu\",padding=\"same\")(x)\n",
        "#x2= Dropout(0.2)(x2)\n",
        "x3 = Conv1D(filters=20,kernel_size=(7),activation=\"relu\",padding=\"same\")(x)\n",
        "#x3= Dropout(0.2)(x3)\n",
        "x4 = Conv1D(filters=20,kernel_size=(9),activation=\"relu\",padding=\"same\")(x)\n",
        "#x4= Dropout(0.2)(x4)\n",
        "x5 = Conv1D(filters=20,kernel_size=(11),activation=\"relu\",padding=\"same\")(x)\n",
        "#x5= Dropout(0.2)(x5)\n",
        "\n",
        "\n",
        "g1 = GlobalAvgPool1D()(x1)\n",
        "g2 = GlobalAvgPool1D()(x2)\n",
        "g3 = GlobalAvgPool1D()(x3)\n",
        "g4 = GlobalAvgPool1D()(x4)\n",
        "g5 = GlobalAvgPool1D()(x5)\n",
        "\n",
        "b1= Concatenate()([g1,g2,g3,g4,g5])\n",
        "#b1 = Dense(2000)(b1)\n",
        "#b1 = BatchNormalization()(b1)\n",
        "#b1 = Activation(\"relu\")(b1)\n",
        "#b1= Dropout(0.3)(b1)\n",
        "#b1 = Dense(4000)(b1)\n",
        "#b1 = BatchNormalization()(b1)\n",
        "#b1 = Activation(\"relu\")(b1)\n",
        "#b1= Dropout(0.3)(b1)\n",
        "out= Dense(1, activation='sigmoid')(b1)\n",
        "model = Model(inputs=a, outputs=out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "## try with flatten after emb and then dense (not good  ~ 66 val acc )\n",
        "## try with globalpool after emb and then dense (works alright ~ 80 val acc )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of the built model...\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           (None, 2381)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_19 (Embedding)        (None, 2381, 30)     2307390     input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 2381, 20)     1820        embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 2381, 20)     3020        embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 2381, 20)     4220        embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 2381, 20)     5420        embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 2381, 20)     6620        embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_11 (Gl (None, 20)           0           conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_12 (Gl (None, 20)           0           conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_13 (Gl (None, 20)           0           conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_14 (Gl (None, 20)           0           conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_15 (Gl (None, 20)           0           conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 100)          0           global_average_pooling1d_11[0][0]\n",
            "                                                                 global_average_pooling1d_12[0][0]\n",
            "                                                                 global_average_pooling1d_13[0][0]\n",
            "                                                                 global_average_pooling1d_14[0][0]\n",
            "                                                                 global_average_pooling1d_15[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 1)            101         concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,328,591\n",
            "Trainable params: 2,328,591\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1y30SLor8pd",
        "colab_type": "code",
        "outputId": "0550c3fc-ffd7-4722-cd63-56e7864c9e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#global max\n",
        "history_1dconv_inception_no_fc_part_no_dropout=model.fit(X_train_pad, y_train, batch_size=64, epochs=5, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "10000/10000 [==============================] - 7s 663us/step - loss: 0.6414 - acc: 0.6819 - val_loss: 0.4991 - val_acc: 0.7901\n",
            "Epoch 2/5\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.3719 - acc: 0.8428 - val_loss: 0.3401 - val_acc: 0.8541\n",
            "Epoch 3/5\n",
            "10000/10000 [==============================] - 5s 490us/step - loss: 0.2046 - acc: 0.9277 - val_loss: 0.3092 - val_acc: 0.8684\n",
            "Epoch 4/5\n",
            "10000/10000 [==============================] - 5s 493us/step - loss: 0.0946 - acc: 0.9739 - val_loss: 0.3239 - val_acc: 0.8655\n",
            "Epoch 5/5\n",
            "10000/10000 [==============================] - 5s 493us/step - loss: 0.0372 - acc: 0.9938 - val_loss: 0.3556 - val_acc: 0.8654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MS7mHJ6r8m-",
        "colab_type": "code",
        "outputId": "4c1f1178-9685-46dc-98e1-0063c385afe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#global ave\n",
        "history_1dconv_inception_no_fc_part_no_dropout=model.fit(X_train_pad, y_train, batch_size=64, epochs=10, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 7s 693us/step - loss: 0.6771 - acc: 0.6129 - val_loss: 0.6187 - val_acc: 0.7738\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 5s 472us/step - loss: 0.4676 - acc: 0.8478 - val_loss: 0.3835 - val_acc: 0.8626\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.2703 - acc: 0.9153 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 5s 475us/step - loss: 0.1764 - acc: 0.9480 - val_loss: 0.2858 - val_acc: 0.8861\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.1207 - acc: 0.9652 - val_loss: 0.2810 - val_acc: 0.8917\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 5s 478us/step - loss: 0.0839 - acc: 0.9798 - val_loss: 0.2872 - val_acc: 0.8898\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.0588 - acc: 0.9878 - val_loss: 0.2966 - val_acc: 0.8915\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.0414 - acc: 0.9928 - val_loss: 0.3077 - val_acc: 0.8905\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 5s 478us/step - loss: 0.0296 - acc: 0.9968 - val_loss: 0.3260 - val_acc: 0.8887\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.0215 - acc: 0.9984 - val_loss: 0.3378 - val_acc: 0.8886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d1549a60-b9c5-4ba2-9660-1da703833e24",
        "id": "gfqP8a_nwNXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation,Dense,CuDNNLSTM, Concatenate, Dropout, Embedding, Conv1D,GlobalMaxPooling1D, Flatten ,MaxPooling1D, GlobalAvgPool1D,BatchNormalization\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "\n",
        "a = Input(shape=(max_length,))\n",
        "\n",
        "x = Embedding(vocab_size+ 1, EMBEDDING_DIM)(a)\n",
        "#x1 = Conv1D(filters=20,kernel_size=(3),activation=\"relu\",padding=\"same\")(x)\n",
        "##x1= Dropout(0.2)(x1)\n",
        "#x2 = Conv1D(filters=20,kernel_size=(5),activation=\"relu\",padding=\"same\")(x)\n",
        "##x2= Dropout(0.2)(x2)\n",
        "#x3 = Conv1D(filters=20,kernel_size=(7),activation=\"relu\",padding=\"same\")(x)\n",
        "##x3= Dropout(0.2)(x3)\n",
        "#x4 = Conv1D(filters=20,kernel_size=(9),activation=\"relu\",padding=\"same\")(x)\n",
        "##x4= Dropout(0.2)(x4)\n",
        "#x5 = Conv1D(filters=20,kernel_size=(11),activation=\"relu\",padding=\"same\")(x)\n",
        "##x5= Dropout(0.2)(x5)\n",
        "#\n",
        "#\n",
        "#g1 = GlobalMaxPooling1D()(x1)\n",
        "#g2 = GlobalMaxPooling1D()(x2)\n",
        "#g3 = GlobalMaxPooling1D()(x3)\n",
        "#g4 = GlobalMaxPooling1D()(x4)\n",
        "#g5 = GlobalMaxPooling1D()(x5)\n",
        "#\n",
        "#b1= Concatenate()([g1,g2,g3,g4,g5])\n",
        "#b1 = Dense(2000)(b1)\n",
        "#b1 = BatchNormalization()(b1)\n",
        "#b1 = Activation(\"relu\")(b1)\n",
        "#b1= Dropout(0.3)(b1)\n",
        "#b1 = Dense(4000)(b1)\n",
        "#b1 = BatchNormalization()(b1)\n",
        "#b1 = Activation(\"relu\")(b1)\n",
        "#b1= Dropout(0.3)(b1)\n",
        "\n",
        "x1 = CuDNNLSTM(20)(x)\n",
        "\n",
        "\n",
        "\n",
        "out= Dense(1, activation='sigmoid')(x1)\n",
        "model = Model(inputs=a, outputs=out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "## try with flatten after emb and then dense (not good  ~ 66 val acc )\n",
        "## try with globalpool after emb and then dense (works alright ~ 80 val acc )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 12:59:38.564084 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0619 12:59:38.589675 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 12:59:38.596537 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0619 12:59:39.959272 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0619 12:59:39.979807 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0619 12:59:39.985745 139948565362560 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Summary of the built model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 2381)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 2381, 30)          2307390   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 20)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 2,311,571\n",
            "Trainable params: 2,311,571\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "607f2ba1-82d4-47ed-84ac-b1db3169e7f8",
        "id": "uMJ0GiJawNXq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "history_1dconv_inception_no_fc_part_no_dropout=model.fit(X_train_pad, y_train, batch_size=64, epochs=80, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 12:59:40.292163 139948565362560 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/80\n",
            "10000/10000 [==============================] - 23s 2ms/step - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.5035\n",
            "Epoch 2/80\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6934 - val_acc: 0.5035\n",
            "Epoch 3/80\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 0.6933 - acc: 0.4970 - val_loss: 0.6933 - val_acc: 0.4965\n",
            "Epoch 4/80\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 0.6933 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.5035\n",
            "Epoch 5/80\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4965\n",
            "Epoch 6/80\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6933 - val_acc: 0.4965\n",
            "Epoch 7/80\n",
            " 5440/10000 [===============>..............] - ETA: 6s - loss: 0.6931 - acc: 0.5088"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b76c41448a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_1dconv_inception_no_fc_part_no_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQVSCGkwMY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRazIfcj1H0A",
        "colab_type": "text"
      },
      "source": [
        "### emb and attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwy9oZy3wMd2",
        "colab_type": "code",
        "outputId": "a6646967-bc9a-49ba-e003-04130f88bf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation,Dense,CuDNNLSTM,RepeatVector,Permute,multiply,Lambda, Concatenate, Dropout, Embedding, Conv1D,GlobalMaxPooling1D, Flatten ,MaxPooling1D, GlobalAvgPool1D,BatchNormalization\n",
        "from keras import backend as K\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "\n",
        "a = Input(shape=(max_length,))\n",
        "\n",
        "x = Embedding(vocab_size+ 1, EMBEDDING_DIM)(a)\n",
        "attention = Dense(10, activation='tanh')(x) \n",
        "attention = Dense(1, activation='tanh')(attention) \n",
        "attention = Flatten()(attention)\n",
        "attention_1 = Activation('softmax')(attention)\n",
        "attention = RepeatVector(30)(attention_1)\n",
        "attention = Permute([2, 1])(attention)\n",
        "\n",
        "sent_representation = multiply([x, attention])\n",
        "sent_representation_1 = Lambda(lambda xin: K.sum(xin, axis=-1))(sent_representation)\n",
        "#sent_representation = Dense(10, activation='relu')(sent_representation)\n",
        "#sent_representation = Flatten()(sent_representation)\n",
        "\n",
        "sent_representation_2 = Dropout(0.5)(sent_representation_1)\n",
        "\n",
        "\n",
        "\n",
        "out= Dense(1, activation='sigmoid')(sent_representation_2)\n",
        "model = Model(inputs=a, outputs=out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "## try with flatten after emb and then dense (not good  ~ 66 val acc )\n",
        "## try with globalpool after emb and then dense (works alright ~ 80 val acc )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of the built model...\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           (None, 2381)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_20 (Embedding)        (None, 2381, 30)     2307390     input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 2381, 10)     310         embedding_20[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 2381, 1)      11          dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_22 (Flatten)            (None, 2381)         0           dense_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 2381)         0           flatten_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_18 (RepeatVector) (None, 30, 2381)     0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "permute_17 (Permute)            (None, 2381, 30)     0           repeat_vector_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "multiply_16 (Multiply)          (None, 2381, 30)     0           embedding_20[0][0]               \n",
            "                                                                 permute_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 2381)         0           multiply_16[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 2381)         0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 1)            2382        dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,310,093\n",
            "Trainable params: 2,310,093\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZuVt0sSwMiq",
        "colab_type": "code",
        "outputId": "05776791-3f37-42fc-e6ea-bb54854b337c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2754
        }
      },
      "source": [
        "#witn tanh\n",
        "history_attention=model.fit(X_train_pad, y_train, batch_size=64, epochs=80, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/80\n",
            "10000/10000 [==============================] - 3s 313us/step - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5035\n",
            "Epoch 2/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5035\n",
            "Epoch 3/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.6924 - acc: 0.5518 - val_loss: 0.6894 - val_acc: 0.7035\n",
            "Epoch 4/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.6778 - acc: 0.7247 - val_loss: 0.6658 - val_acc: 0.7542\n",
            "Epoch 5/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.6439 - acc: 0.7694 - val_loss: 0.6323 - val_acc: 0.7622\n",
            "Epoch 6/80\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.6061 - acc: 0.7828 - val_loss: 0.5987 - val_acc: 0.7800\n",
            "Epoch 7/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.5709 - acc: 0.7883 - val_loss: 0.5688 - val_acc: 0.7935\n",
            "Epoch 8/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.5395 - acc: 0.8068 - val_loss: 0.5423 - val_acc: 0.7971\n",
            "Epoch 9/80\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.5100 - acc: 0.8117 - val_loss: 0.5179 - val_acc: 0.8064\n",
            "Epoch 10/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.4889 - acc: 0.8145 - val_loss: 0.4980 - val_acc: 0.8125\n",
            "Epoch 11/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.4661 - acc: 0.8220 - val_loss: 0.4804 - val_acc: 0.8186\n",
            "Epoch 12/80\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.4467 - acc: 0.8305 - val_loss: 0.4647 - val_acc: 0.8255\n",
            "Epoch 13/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.4295 - acc: 0.8399 - val_loss: 0.4509 - val_acc: 0.8283\n",
            "Epoch 14/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.4166 - acc: 0.8422 - val_loss: 0.4384 - val_acc: 0.8342\n",
            "Epoch 15/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.4013 - acc: 0.8474 - val_loss: 0.4270 - val_acc: 0.8381\n",
            "Epoch 16/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.3873 - acc: 0.8584 - val_loss: 0.4167 - val_acc: 0.8416\n",
            "Epoch 17/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.3749 - acc: 0.8651 - val_loss: 0.4071 - val_acc: 0.8449\n",
            "Epoch 18/80\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.3670 - acc: 0.8636 - val_loss: 0.3975 - val_acc: 0.8491\n",
            "Epoch 19/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.3556 - acc: 0.8705 - val_loss: 0.3894 - val_acc: 0.8525\n",
            "Epoch 20/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.3436 - acc: 0.8756 - val_loss: 0.3819 - val_acc: 0.8564\n",
            "Epoch 21/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.3368 - acc: 0.8810 - val_loss: 0.3750 - val_acc: 0.8586\n",
            "Epoch 22/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.3288 - acc: 0.8797 - val_loss: 0.3686 - val_acc: 0.8614\n",
            "Epoch 23/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.3188 - acc: 0.8855 - val_loss: 0.3627 - val_acc: 0.8630\n",
            "Epoch 24/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.3137 - acc: 0.8850 - val_loss: 0.3574 - val_acc: 0.8651\n",
            "Epoch 25/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.3041 - acc: 0.8912 - val_loss: 0.3523 - val_acc: 0.8658\n",
            "Epoch 26/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.3020 - acc: 0.8902 - val_loss: 0.3471 - val_acc: 0.8689\n",
            "Epoch 27/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.2896 - acc: 0.8959 - val_loss: 0.3423 - val_acc: 0.8713\n",
            "Epoch 28/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.2875 - acc: 0.8980 - val_loss: 0.3379 - val_acc: 0.8730\n",
            "Epoch 29/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.2815 - acc: 0.9011 - val_loss: 0.3339 - val_acc: 0.8740\n",
            "Epoch 30/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.2784 - acc: 0.9015 - val_loss: 0.3300 - val_acc: 0.8734\n",
            "Epoch 31/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.2694 - acc: 0.9044 - val_loss: 0.3269 - val_acc: 0.8750\n",
            "Epoch 32/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.2630 - acc: 0.9080 - val_loss: 0.3237 - val_acc: 0.8757\n",
            "Epoch 33/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.2553 - acc: 0.9106 - val_loss: 0.3205 - val_acc: 0.8772\n",
            "Epoch 34/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.2518 - acc: 0.9103 - val_loss: 0.3176 - val_acc: 0.8785\n",
            "Epoch 35/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.2436 - acc: 0.9174 - val_loss: 0.3149 - val_acc: 0.8785\n",
            "Epoch 36/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.2436 - acc: 0.9157 - val_loss: 0.3122 - val_acc: 0.8798\n",
            "Epoch 37/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.2386 - acc: 0.9170 - val_loss: 0.3100 - val_acc: 0.8803\n",
            "Epoch 38/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.2338 - acc: 0.9212 - val_loss: 0.3077 - val_acc: 0.8815\n",
            "Epoch 39/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.2297 - acc: 0.9222 - val_loss: 0.3053 - val_acc: 0.8826\n",
            "Epoch 40/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.2279 - acc: 0.9206 - val_loss: 0.3033 - val_acc: 0.8827\n",
            "Epoch 41/80\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.2253 - acc: 0.9211 - val_loss: 0.3010 - val_acc: 0.8827\n",
            "Epoch 42/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.2209 - acc: 0.9224 - val_loss: 0.2993 - val_acc: 0.8822\n",
            "Epoch 43/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.2141 - acc: 0.9293 - val_loss: 0.2972 - val_acc: 0.8835\n",
            "Epoch 44/80\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.2144 - acc: 0.9292 - val_loss: 0.2956 - val_acc: 0.8835\n",
            "Epoch 45/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.2065 - acc: 0.9338 - val_loss: 0.2941 - val_acc: 0.8829\n",
            "Epoch 46/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.2026 - acc: 0.9314 - val_loss: 0.2925 - val_acc: 0.8839\n",
            "Epoch 47/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.2005 - acc: 0.9338 - val_loss: 0.2911 - val_acc: 0.8844\n",
            "Epoch 48/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1955 - acc: 0.9346 - val_loss: 0.2899 - val_acc: 0.8849\n",
            "Epoch 49/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1995 - acc: 0.9319 - val_loss: 0.2887 - val_acc: 0.8851\n",
            "Epoch 50/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.1938 - acc: 0.9355 - val_loss: 0.2874 - val_acc: 0.8860\n",
            "Epoch 51/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1894 - acc: 0.9369 - val_loss: 0.2863 - val_acc: 0.8861\n",
            "Epoch 52/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1887 - acc: 0.9362 - val_loss: 0.2851 - val_acc: 0.8865\n",
            "Epoch 53/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1868 - acc: 0.9384 - val_loss: 0.2838 - val_acc: 0.8864\n",
            "Epoch 54/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.1798 - acc: 0.9398 - val_loss: 0.2826 - val_acc: 0.8872\n",
            "Epoch 55/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.1785 - acc: 0.9413 - val_loss: 0.2817 - val_acc: 0.8872\n",
            "Epoch 56/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1744 - acc: 0.9431 - val_loss: 0.2808 - val_acc: 0.8870\n",
            "Epoch 57/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1759 - acc: 0.9414 - val_loss: 0.2798 - val_acc: 0.8880\n",
            "Epoch 58/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1679 - acc: 0.9472 - val_loss: 0.2790 - val_acc: 0.8877\n",
            "Epoch 59/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1658 - acc: 0.9458 - val_loss: 0.2784 - val_acc: 0.8887\n",
            "Epoch 60/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1658 - acc: 0.9466 - val_loss: 0.2779 - val_acc: 0.8880\n",
            "Epoch 61/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.1618 - acc: 0.9469 - val_loss: 0.2769 - val_acc: 0.8880\n",
            "Epoch 62/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1631 - acc: 0.9470 - val_loss: 0.2759 - val_acc: 0.8882\n",
            "Epoch 63/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1595 - acc: 0.9463 - val_loss: 0.2754 - val_acc: 0.8884\n",
            "Epoch 64/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1592 - acc: 0.9476 - val_loss: 0.2749 - val_acc: 0.8882\n",
            "Epoch 65/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.1507 - acc: 0.9529 - val_loss: 0.2740 - val_acc: 0.8884\n",
            "Epoch 66/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1518 - acc: 0.9505 - val_loss: 0.2739 - val_acc: 0.8882\n",
            "Epoch 67/80\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.1471 - acc: 0.9528 - val_loss: 0.2733 - val_acc: 0.8891\n",
            "Epoch 68/80\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.1459 - acc: 0.9547 - val_loss: 0.2728 - val_acc: 0.8887\n",
            "Epoch 69/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1460 - acc: 0.9538 - val_loss: 0.2724 - val_acc: 0.8888\n",
            "Epoch 70/80\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.1441 - acc: 0.9533 - val_loss: 0.2721 - val_acc: 0.8885\n",
            "Epoch 71/80\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.1400 - acc: 0.9530 - val_loss: 0.2718 - val_acc: 0.8882\n",
            "Epoch 72/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1428 - acc: 0.9550 - val_loss: 0.2715 - val_acc: 0.8896\n",
            "Epoch 73/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.1337 - acc: 0.9586 - val_loss: 0.2709 - val_acc: 0.8891\n",
            "Epoch 74/80\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.1414 - acc: 0.9534 - val_loss: 0.2707 - val_acc: 0.8890\n",
            "Epoch 75/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1341 - acc: 0.9573 - val_loss: 0.2703 - val_acc: 0.8890\n",
            "Epoch 76/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1341 - acc: 0.9585 - val_loss: 0.2701 - val_acc: 0.8883\n",
            "Epoch 77/80\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.1315 - acc: 0.9585 - val_loss: 0.2698 - val_acc: 0.8878\n",
            "Epoch 78/80\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.1294 - acc: 0.9604 - val_loss: 0.2697 - val_acc: 0.8888\n",
            "Epoch 79/80\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.1285 - acc: 0.9600 - val_loss: 0.2694 - val_acc: 0.8883\n",
            "Epoch 80/80\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.1219 - acc: 0.9609 - val_loss: 0.2696 - val_acc: 0.8886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv2g74wz3gGi",
        "colab_type": "code",
        "outputId": "e9687463-fda6-4a0d-99a8-89ced1908718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4029
        }
      },
      "source": [
        "#with relu does not work!\n",
        "#history_attention=model.fit(X_train_pad, y_train, batch_size=64, epochs=120, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/120\n",
            "10000/10000 [==============================] - 2s 180us/step - loss: 0.0851 - acc: 0.9836 - val_loss: 0.2867 - val_acc: 0.8824\n",
            "Epoch 2/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0833 - acc: 0.9846 - val_loss: 0.2874 - val_acc: 0.8818\n",
            "Epoch 3/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0815 - acc: 0.9846 - val_loss: 0.2870 - val_acc: 0.8828\n",
            "Epoch 4/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0797 - acc: 0.9852 - val_loss: 0.2868 - val_acc: 0.8831\n",
            "Epoch 5/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0780 - acc: 0.9857 - val_loss: 0.2878 - val_acc: 0.8817\n",
            "Epoch 6/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0763 - acc: 0.9858 - val_loss: 0.2871 - val_acc: 0.8830\n",
            "Epoch 7/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0745 - acc: 0.9873 - val_loss: 0.2884 - val_acc: 0.8809\n",
            "Epoch 8/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0729 - acc: 0.9875 - val_loss: 0.2905 - val_acc: 0.8806\n",
            "Epoch 9/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0713 - acc: 0.9878 - val_loss: 0.2891 - val_acc: 0.8804\n",
            "Epoch 10/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0698 - acc: 0.9880 - val_loss: 0.2893 - val_acc: 0.8803\n",
            "Epoch 11/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0681 - acc: 0.9890 - val_loss: 0.2888 - val_acc: 0.8821\n",
            "Epoch 12/120\n",
            "10000/10000 [==============================] - 2s 172us/step - loss: 0.0666 - acc: 0.9890 - val_loss: 0.2892 - val_acc: 0.8817\n",
            "Epoch 13/120\n",
            "10000/10000 [==============================] - 2s 174us/step - loss: 0.0651 - acc: 0.9894 - val_loss: 0.2898 - val_acc: 0.8811\n",
            "Epoch 14/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0637 - acc: 0.9902 - val_loss: 0.2896 - val_acc: 0.8814\n",
            "Epoch 15/120\n",
            "10000/10000 [==============================] - 2s 172us/step - loss: 0.0623 - acc: 0.9901 - val_loss: 0.2911 - val_acc: 0.8809\n",
            "Epoch 16/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0610 - acc: 0.9906 - val_loss: 0.2919 - val_acc: 0.8808\n",
            "Epoch 17/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0597 - acc: 0.9907 - val_loss: 0.2926 - val_acc: 0.8802\n",
            "Epoch 18/120\n",
            "10000/10000 [==============================] - 2s 172us/step - loss: 0.0582 - acc: 0.9912 - val_loss: 0.2925 - val_acc: 0.8808\n",
            "Epoch 19/120\n",
            "10000/10000 [==============================] - 2s 173us/step - loss: 0.0569 - acc: 0.9916 - val_loss: 0.2919 - val_acc: 0.8804\n",
            "Epoch 20/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0557 - acc: 0.9919 - val_loss: 0.2922 - val_acc: 0.8798\n",
            "Epoch 21/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0544 - acc: 0.9918 - val_loss: 0.2939 - val_acc: 0.8805\n",
            "Epoch 22/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0532 - acc: 0.9922 - val_loss: 0.2937 - val_acc: 0.8804\n",
            "Epoch 23/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0519 - acc: 0.9926 - val_loss: 0.2965 - val_acc: 0.8804\n",
            "Epoch 24/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0507 - acc: 0.9931 - val_loss: 0.2947 - val_acc: 0.8801\n",
            "Epoch 25/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0496 - acc: 0.9937 - val_loss: 0.2961 - val_acc: 0.8802\n",
            "Epoch 26/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0484 - acc: 0.9936 - val_loss: 0.2965 - val_acc: 0.8803\n",
            "Epoch 27/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0473 - acc: 0.9941 - val_loss: 0.2980 - val_acc: 0.8796\n",
            "Epoch 28/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0463 - acc: 0.9942 - val_loss: 0.2987 - val_acc: 0.8791\n",
            "Epoch 29/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0452 - acc: 0.9947 - val_loss: 0.2983 - val_acc: 0.8806\n",
            "Epoch 30/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0441 - acc: 0.9951 - val_loss: 0.2991 - val_acc: 0.8804\n",
            "Epoch 31/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0431 - acc: 0.9951 - val_loss: 0.3012 - val_acc: 0.8791\n",
            "Epoch 32/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0421 - acc: 0.9957 - val_loss: 0.3020 - val_acc: 0.8790\n",
            "Epoch 33/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0411 - acc: 0.9959 - val_loss: 0.3026 - val_acc: 0.8789\n",
            "Epoch 34/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0402 - acc: 0.9962 - val_loss: 0.3021 - val_acc: 0.8795\n",
            "Epoch 35/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0393 - acc: 0.9961 - val_loss: 0.3032 - val_acc: 0.8795\n",
            "Epoch 36/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0383 - acc: 0.9968 - val_loss: 0.3035 - val_acc: 0.8789\n",
            "Epoch 37/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0374 - acc: 0.9965 - val_loss: 0.3041 - val_acc: 0.8788\n",
            "Epoch 38/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0366 - acc: 0.9968 - val_loss: 0.3061 - val_acc: 0.8784\n",
            "Epoch 39/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0357 - acc: 0.9969 - val_loss: 0.3060 - val_acc: 0.8789\n",
            "Epoch 40/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0349 - acc: 0.9970 - val_loss: 0.3064 - val_acc: 0.8787\n",
            "Epoch 41/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0340 - acc: 0.9970 - val_loss: 0.3093 - val_acc: 0.8783\n",
            "Epoch 42/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0332 - acc: 0.9974 - val_loss: 0.3088 - val_acc: 0.8781\n",
            "Epoch 43/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0324 - acc: 0.9975 - val_loss: 0.3095 - val_acc: 0.8783\n",
            "Epoch 44/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0317 - acc: 0.9973 - val_loss: 0.3117 - val_acc: 0.8781\n",
            "Epoch 45/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0309 - acc: 0.9978 - val_loss: 0.3123 - val_acc: 0.8778\n",
            "Epoch 46/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0302 - acc: 0.9979 - val_loss: 0.3120 - val_acc: 0.8781\n",
            "Epoch 47/120\n",
            "10000/10000 [==============================] - 2s 166us/step - loss: 0.0295 - acc: 0.9978 - val_loss: 0.3142 - val_acc: 0.8777\n",
            "Epoch 48/120\n",
            "10000/10000 [==============================] - 2s 166us/step - loss: 0.0287 - acc: 0.9980 - val_loss: 0.3171 - val_acc: 0.8764\n",
            "Epoch 49/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0281 - acc: 0.9980 - val_loss: 0.3156 - val_acc: 0.8778\n",
            "Epoch 50/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0274 - acc: 0.9981 - val_loss: 0.3168 - val_acc: 0.8773\n",
            "Epoch 51/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0267 - acc: 0.9982 - val_loss: 0.3173 - val_acc: 0.8781\n",
            "Epoch 52/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0261 - acc: 0.9982 - val_loss: 0.3198 - val_acc: 0.8761\n",
            "Epoch 53/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0255 - acc: 0.9981 - val_loss: 0.3192 - val_acc: 0.8780\n",
            "Epoch 54/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0249 - acc: 0.9983 - val_loss: 0.3218 - val_acc: 0.8757\n",
            "Epoch 55/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0243 - acc: 0.9984 - val_loss: 0.3239 - val_acc: 0.8754\n",
            "Epoch 56/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0236 - acc: 0.9985 - val_loss: 0.3257 - val_acc: 0.8750\n",
            "Epoch 57/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0231 - acc: 0.9985 - val_loss: 0.3244 - val_acc: 0.8761\n",
            "Epoch 58/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0225 - acc: 0.9987 - val_loss: 0.3256 - val_acc: 0.8758\n",
            "Epoch 59/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0220 - acc: 0.9988 - val_loss: 0.3279 - val_acc: 0.8757\n",
            "Epoch 60/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0215 - acc: 0.9987 - val_loss: 0.3273 - val_acc: 0.8762\n",
            "Epoch 61/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0209 - acc: 0.9988 - val_loss: 0.3306 - val_acc: 0.8748\n",
            "Epoch 62/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0204 - acc: 0.9989 - val_loss: 0.3305 - val_acc: 0.8748\n",
            "Epoch 63/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0199 - acc: 0.9988 - val_loss: 0.3322 - val_acc: 0.8745\n",
            "Epoch 64/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0194 - acc: 0.9989 - val_loss: 0.3336 - val_acc: 0.8746\n",
            "Epoch 65/120\n",
            "10000/10000 [==============================] - 2s 173us/step - loss: 0.0189 - acc: 0.9988 - val_loss: 0.3336 - val_acc: 0.8757\n",
            "Epoch 66/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0184 - acc: 0.9990 - val_loss: 0.3341 - val_acc: 0.8765\n",
            "Epoch 67/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0180 - acc: 0.9988 - val_loss: 0.3367 - val_acc: 0.8736\n",
            "Epoch 68/120\n",
            "10000/10000 [==============================] - 2s 171us/step - loss: 0.0176 - acc: 0.9990 - val_loss: 0.3370 - val_acc: 0.8755\n",
            "Epoch 69/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0171 - acc: 0.9990 - val_loss: 0.3383 - val_acc: 0.8751\n",
            "Epoch 70/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0167 - acc: 0.9989 - val_loss: 0.3402 - val_acc: 0.8742\n",
            "Epoch 71/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0163 - acc: 0.9991 - val_loss: 0.3421 - val_acc: 0.8734\n",
            "Epoch 72/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0158 - acc: 0.9992 - val_loss: 0.3424 - val_acc: 0.8742\n",
            "Epoch 73/120\n",
            "10000/10000 [==============================] - 2s 166us/step - loss: 0.0154 - acc: 0.9991 - val_loss: 0.3485 - val_acc: 0.8718\n",
            "Epoch 74/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0151 - acc: 0.9991 - val_loss: 0.3481 - val_acc: 0.8730\n",
            "Epoch 75/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0147 - acc: 0.9994 - val_loss: 0.3471 - val_acc: 0.8739\n",
            "Epoch 76/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0143 - acc: 0.9994 - val_loss: 0.3482 - val_acc: 0.8741\n",
            "Epoch 77/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0139 - acc: 0.9994 - val_loss: 0.3498 - val_acc: 0.8739\n",
            "Epoch 78/120\n",
            "10000/10000 [==============================] - 2s 166us/step - loss: 0.0136 - acc: 0.9995 - val_loss: 0.3509 - val_acc: 0.8739\n",
            "Epoch 79/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0132 - acc: 0.9996 - val_loss: 0.3544 - val_acc: 0.8725\n",
            "Epoch 80/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0129 - acc: 0.9996 - val_loss: 0.3553 - val_acc: 0.8731\n",
            "Epoch 81/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0126 - acc: 0.9996 - val_loss: 0.3542 - val_acc: 0.8732\n",
            "Epoch 82/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0123 - acc: 0.9996 - val_loss: 0.3578 - val_acc: 0.8721\n",
            "Epoch 83/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0119 - acc: 0.9996 - val_loss: 0.3581 - val_acc: 0.8733\n",
            "Epoch 84/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0116 - acc: 0.9996 - val_loss: 0.3597 - val_acc: 0.8729\n",
            "Epoch 85/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0113 - acc: 0.9996 - val_loss: 0.3604 - val_acc: 0.8725\n",
            "Epoch 86/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0110 - acc: 0.9996 - val_loss: 0.3649 - val_acc: 0.8703\n",
            "Epoch 87/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0108 - acc: 0.9998 - val_loss: 0.3638 - val_acc: 0.8726\n",
            "Epoch 88/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0105 - acc: 0.9997 - val_loss: 0.3650 - val_acc: 0.8726\n",
            "Epoch 89/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0102 - acc: 0.9997 - val_loss: 0.3660 - val_acc: 0.8727\n",
            "Epoch 90/120\n",
            "10000/10000 [==============================] - 2s 170us/step - loss: 0.0099 - acc: 0.9998 - val_loss: 0.3674 - val_acc: 0.8724\n",
            "Epoch 91/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0097 - acc: 0.9998 - val_loss: 0.3691 - val_acc: 0.8721\n",
            "Epoch 92/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0094 - acc: 0.9998 - val_loss: 0.3717 - val_acc: 0.8706\n",
            "Epoch 93/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0092 - acc: 0.9998 - val_loss: 0.3720 - val_acc: 0.8724\n",
            "Epoch 94/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0089 - acc: 0.9998 - val_loss: 0.3747 - val_acc: 0.8704\n",
            "Epoch 95/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0087 - acc: 0.9998 - val_loss: 0.3757 - val_acc: 0.8708\n",
            "Epoch 96/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0085 - acc: 0.9998 - val_loss: 0.3777 - val_acc: 0.8705\n",
            "Epoch 97/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0082 - acc: 0.9998 - val_loss: 0.3798 - val_acc: 0.8704\n",
            "Epoch 98/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0080 - acc: 0.9998 - val_loss: 0.3809 - val_acc: 0.8704\n",
            "Epoch 99/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0078 - acc: 0.9999 - val_loss: 0.3824 - val_acc: 0.8697\n",
            "Epoch 100/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0076 - acc: 0.9998 - val_loss: 0.3836 - val_acc: 0.8708\n",
            "Epoch 101/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0074 - acc: 0.9999 - val_loss: 0.3863 - val_acc: 0.8698\n",
            "Epoch 102/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0072 - acc: 0.9999 - val_loss: 0.3874 - val_acc: 0.8700\n",
            "Epoch 103/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0070 - acc: 0.9999 - val_loss: 0.3907 - val_acc: 0.8684\n",
            "Epoch 104/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0068 - acc: 0.9999 - val_loss: 0.3907 - val_acc: 0.8699\n",
            "Epoch 105/120\n",
            "10000/10000 [==============================] - 2s 166us/step - loss: 0.0066 - acc: 0.9999 - val_loss: 0.3908 - val_acc: 0.8711\n",
            "Epoch 106/120\n",
            "10000/10000 [==============================] - 2s 168us/step - loss: 0.0065 - acc: 0.9999 - val_loss: 0.3954 - val_acc: 0.8682\n",
            "Epoch 107/120\n",
            "10000/10000 [==============================] - 2s 169us/step - loss: 0.0063 - acc: 0.9999 - val_loss: 0.3944 - val_acc: 0.8701\n",
            "Epoch 108/120\n",
            "10000/10000 [==============================] - 2s 167us/step - loss: 0.0061 - acc: 0.9999 - val_loss: 0.3984 - val_acc: 0.8684\n",
            "Epoch 109/120\n",
            " 2624/10000 [======>.......................] - ETA: 0s - loss: 0.0058 - acc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-0cb09bc2fd35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BVTwphlwMnw",
        "colab_type": "code",
        "outputId": "aa1f6125-fac4-41d3-af17-e6317dbb6422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "y_pred = (model.predict(X_test_pad)>0.5)*1\n",
        "y_pred=y_pred.reshape(len(y_pred))\n",
        "print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n",
        "print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred     0     1\n",
            "true            \n",
            "0     2234   273\n",
            "1      263  2230\n",
            "Acc =  0.8928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkCIJaW3wMwM",
        "colab_type": "code",
        "outputId": "88f32cd3-abb9-4574-c570-c2384c4d18ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X[5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Only watched this to see Joe Morton in an early role and honestly wished I hadn't bothered, he can and has since, done much better than this crap. Cannot understand why anyone finds this kind of stupidity funny but each to his own; it is an absolute mess and not funny in the least. No wait, ONE line only was funny, where Mr Kent (Joe) and his family are having dinner with this nut job as he's been invited for dinner (Lord alone knows why). Pest to Mr Kent: You know what it's like dog, you've been there Mrs Kent: Not lately, Joe's expression was funny but that's it one line does not make a great comedy and this tat is so far away from being funny it should be consigned to the nearest trash cart, it's only good enough for that. Joe Morton - glad to see you don't appear in rubbish like this anymore; you are far superior and a great great actor.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlabaVGTDWPP",
        "colab_type": "code",
        "outputId": "3e5d1ead-64e1-46e1-a45b-28697232e5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "853"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-VnN1DjwM1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_attention_weights=Model(inputs=a, outputs=attention_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0DgJ6VYwM5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights=model_attention_weights.predict(X_train_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2QUULSG_yft",
        "colab_type": "code",
        "outputId": "77ecf10f-4e80-44a9-a3f2-95f53a10b9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(model.predict(X_train_pad[5:6]))\n",
        "print(y[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.05689497]]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GGrGEL4wNCQ",
        "colab_type": "code",
        "outputId": "f8bb3aee-f0e5-498a-9338-578f42e59505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5A54L2AwNGe",
        "colab_type": "code",
        "outputId": "01b8039c-3a47-4e98-b895-2ad05fd979e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "weights[5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00254924, 0.00254924, 0.00034501, ..., 0.000345  , 0.000345  ,\n",
              "       0.000345  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU-NhFPwr8kQ",
        "colab_type": "code",
        "outputId": "c7b3cd06-2963-4640-eaf7-7f20f747026c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "plt.hist(weights[5],bins=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2300.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
              "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
              "           0.,   81.]),\n",
              " array([0.000345  , 0.00045521, 0.00056543, 0.00067564, 0.00078585,\n",
              "        0.00089606, 0.00100627, 0.00111648, 0.0012267 , 0.00133691,\n",
              "        0.00144712, 0.00155733, 0.00166754, 0.00177775, 0.00188797,\n",
              "        0.00199818, 0.00210839, 0.0022186 , 0.00232881, 0.00243902,\n",
              "        0.00254924], dtype=float32),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasoAkKSmDABJasoAkKSm/g9bkdBcjmLpOAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaBKjyefr8hR",
        "colab_type": "code",
        "outputId": "f20836e9-2220-4fe4-9129-abfacc97e900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "plt.hist(weights[5][-len(X[5]):],bins=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 853.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
              " array([-4.9965501e-01, -4.4965500e-01, -3.9965498e-01, -3.4965500e-01,\n",
              "        -2.9965499e-01, -2.4965499e-01, -1.9965500e-01, -1.4965500e-01,\n",
              "        -9.9654995e-02, -4.9654994e-02,  3.4500423e-04,  5.0345004e-02,\n",
              "         1.0034500e-01,  1.5034500e-01,  2.0034501e-01,  2.5034499e-01,\n",
              "         3.0034500e-01,  3.5034502e-01,  4.0034500e-01,  4.5034501e-01,\n",
              "         5.0034499e-01], dtype=float32),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "513Wk8sGDluf",
        "colab_type": "code",
        "outputId": "b5c0ef03-60f3-42b9-d05c-d817abb9bf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique(weights[5][-len(X[5]):])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.000345], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb8cnnbLAZyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt5eEkfGAZvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DgqDLfyAZsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKAbiCuVAZpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhrS0yPLOfgI",
        "colab_type": "text"
      },
      "source": [
        "prediction on the test set with the accuracy and the confusion matrix. Are we better than the RF baseline?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_a9qkStOfgN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### We need to go deeper! \n",
        "Write your own review or test the model on some reviews of Inception or some other movie, note that we can use an arbitrary length for the review.  \n",
        "Visit: https://www.imdb.com/title/tt1375666/reviews?ref_=tt_ov_rt  \n",
        "remeber 0 is negative and 1 is postive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWmcDquOfgO",
        "colab_type": "code",
        "outputId": "d1633d51-198e-4063-d3cb-cfea8447ab28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#paste new reviews here in the sample list\n",
        "sample=list([\"Inception is truly one of a kind. A concept which has long gestated in Christopher Nolan's mind, his eye for drama mixed with his large scale sensibilities ring true in Blockbuster season making Inception a true original in the sea of reboots, remakes and sequels.To try and explain Inceptions many plot twists and incredibly intelligent arcs, would be a foolish task. As Nolan himself has been reluctant to. The best way to approach the film would be with an open mind, if you are prepared to be taken on a ride of a lifetime, then trust that you 100% will. If Avatar was a seminal film in technology (although coming out as a rather poor film, in my opinion), then Inception is seminal in it's storytelling. With a 148 minute running time, you would expect a lot to take place, but what you wouldn't expect is the pace of it all. I did not think at one time in the film about how long was left. I was simply blown away by the depth in every single part of the film. If my enthusiasm for the storytelling aspect of the film has left you worried about the spectacle, then don't worry. They are, as hinted in the trailer, incredible, looking real and unbelievable simultaneously. The most pleasing thing about the action set pieces, is that they are genuinely used to illustrate the story, rather than to blow stuff up a la Michael Bay. With this complex movie in it's high concept, a stellar cast is needed. And Nolan as always, delivers with just that. This is vintage DiCaprio, perhaps only equalled in The Aviator, which is even more impressive as his role as Cobb in Inception is not a showy one, needing DiCaprio to be the constant at the centre of the film. And he pulls off Cobb's emotional contradictions sublimely. The rest of the cast members all shine in parts of the films, Cillian Murphy shows off his usually non-existent tender side, Gordon-Levitt bottles his usual charm for his confidently reserved turn as the reliable Arthur, Watanabe is devilish as the seemingly ambiguous Saito, Page shows why she's the next big female star and Tom Hardy revels in being the comic relief of the film compared to his recent turns as decidedly psychopathic characters. Overall, Nolan has indeed surpassed himself. He has created a world as expansive as his Gotham, a plot dwarfing the intricacies of Memento and one which blows The Prestige's cinematic reveal out of the water. This is truly unmissable cinema. Revel in it, we've still got to wait a whole two years before Batman 3.\",\n",
        "             \"Based on reviews I was hoping this was a different American film in the sense that it will have substance, subtlety and that it will make me think. It did not, it did not and it did not again. It is your typical Hollywood flick with car chases, shooting galore, explosions, fistfights, pretty boys and girls - the whole nine yards to sell tickets to the ADHD generation of teenagers. I gave it 3 stars - instead of just 2 - because the special effects are absolutely astonishing. This film wishes to be clever but really is not and does not make a lot of sense either. It reminded me of the teachers/professors who were confusing on purpose in order to make us believe they are smarter than we were.\"])\n",
        "sample_tokens=tokenizer.texts_to_sequences(sample)\n",
        "#print(np.array(sample_tokens))\n",
        "print(model.predict(np.array(sample_tokens[0:1])))\n",
        "print(model.predict(np.array(sample_tokens[1:2])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-cacb51cf0df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(np.array(sample_tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_20 to have shape (2381,) but got array with shape (432,)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phMBGCA0OfgR",
        "colab_type": "text"
      },
      "source": [
        "### Optional: Neural network with wordembedding (inception like) with Keras functional api  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxlHDe18OfgS",
        "colab_type": "code",
        "outputId": "2d32fe00-9856-415b-efac-67f2d3811343",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Concatenate, Dropout, Embedding, Conv1D,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "EMBEDDING_DIM = 30\n",
        "\n",
        "\n",
        "a = Input(shape=(max_length,))\n",
        "x = Embedding(vocab_size, EMBEDDING_DIM)(a)\n",
        "x1 = Conv1D(filters=50,kernel_size=(3),activation=\"relu\",padding=\"same\")(x)\n",
        "x2 = Conv1D(filters=50,kernel_size=(5),activation=\"relu\",padding=\"same\")(x)\n",
        "x3 = Conv1D(filters=50,kernel_size=(7),activation=\"relu\",padding=\"same\")(x)\n",
        "\n",
        "g1 = GlobalAveragePooling1D()(x1)\n",
        "g2 = GlobalAveragePooling1D()(x2)\n",
        "g3 = GlobalAveragePooling1D()(x3)\n",
        "conc= Concatenate()([g1,g2,g3])\n",
        "conc = Dropout(0.3)(conc)\n",
        "conc = Dense(50, activation='relu')(conc)\n",
        "conc = Dropout(0.3)(conc)\n",
        "out= Dense(1, activation='sigmoid')(conc)\n",
        "model = Model(inputs=a, outputs=out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`NHWC` for data_format is deprecated, use `NWC` instead\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 2381)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 2381, 30)     1868910     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 2381, 50)     4550        embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 2381, 50)     7550        embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 2381, 50)     10550       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 50)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 50)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_4 (Glo (None, 50)           0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 150)          0           global_average_pooling1d_2[0][0] \n",
            "                                                                 global_average_pooling1d_3[0][0] \n",
            "                                                                 global_average_pooling1d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 150)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 50)           7550        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 50)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            51          dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,899,161\n",
            "Trainable params: 1,899,161\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaFMf-8EOfgW",
        "colab_type": "code",
        "outputId": "457e1edb-6ed7-4f50-947d-3eccc1b48782",
        "colab": {}
      },
      "source": [
        "history=model.fit(X_train_pad, y_train, batch_size=64, epochs=10, validation_data=(X_val_pad, y_val), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 134s 27ms/step - loss: 0.6932 - acc: 0.5112 - val_loss: 0.6917 - val_acc: 0.5066\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 134s 27ms/step - loss: 0.6681 - acc: 0.6152 - val_loss: 0.6030 - val_acc: 0.7400\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 136s 27ms/step - loss: 0.4087 - acc: 0.8516 - val_loss: 0.3498 - val_acc: 0.8606\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 131s 26ms/step - loss: 0.1911 - acc: 0.9340 - val_loss: 0.3343 - val_acc: 0.8676\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 137s 27ms/step - loss: 0.1122 - acc: 0.9638 - val_loss: 0.3568 - val_acc: 0.8668\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 135s 27ms/step - loss: 0.0660 - acc: 0.9808 - val_loss: 0.3933 - val_acc: 0.8650\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 134s 27ms/step - loss: 0.0432 - acc: 0.9896 - val_loss: 0.4086 - val_acc: 0.8654\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 133s 27ms/step - loss: 0.0259 - acc: 0.9962 - val_loss: 0.4439 - val_acc: 0.8652\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 133s 27ms/step - loss: 0.0177 - acc: 0.9966 - val_loss: 0.4750 - val_acc: 0.8652\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 132s 26ms/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.5142 - val_acc: 0.8660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63SJzB8AOfgZ",
        "colab_type": "code",
        "outputId": "6e2e949a-d881-4981-b58a-7c32a4812c8e",
        "colab": {}
      },
      "source": [
        "# summarize history \n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f236015f4a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f236fb19160>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batRfbrgOfgc",
        "colab_type": "code",
        "outputId": "aff4d3b3-bcda-4cbe-fa8f-2f3aa7285649",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test_pad)  \n",
        "y_pred=np.reshape(np.round(y_pred,0),(len(y_pred)))\n",
        "print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n",
        "print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred   0.0   1.0\n",
            "true            \n",
            "0     2178   233\n",
            "1      371  2218\n",
            "Acc =  0.8792\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}